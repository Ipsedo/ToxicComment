{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data.read_data as read\n",
    "import data.transform_data as transform\n",
    "from numpy import concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_file_name = \"res/train.csv\"\n",
    "test_csv_file_name = \"res/test.csv\"\n",
    "test_label_csv_file_name = \"res/test_labels.csv\"\n",
    "submission_csv_file_name = \"res/sample_submission.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = read.load_train_csv(train_csv_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      id                                       comment_text  \\\n0       0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n1       000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n2       000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n3       0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n4       0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n5       00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...   \n6       0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n7       00031b1e95af7921  Your vandalism to the Matt Shirvington article...   \n8       00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...   \n9       00040093b2687caa  alignment on this subject and which are contra...   \n10      0005300084f90edc  \"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...   \n11      00054a5e18b50dd4  bbq \\n\\nbe a man and lets discuss it-maybe ove...   \n12      0005c987bdfc9d4b  Hey... what is it..\\n@ | talk .\\nWhat is it......   \n13      0006f16e4e9f292e  Before you start throwing accusations and warn...   \n14      00070ef96486d6f9  Oh, and the girl above started her arguments w...   \n15      00078f8ce7eb276d  \"\\n\\nJuelz Santanas Age\\n\\nIn 2002, Juelz Sant...   \n16      0007e25b2121310b  Bye! \\n\\nDon't look, come or think of comming ...   \n17      000897889268bc93   REDIRECT Talk:Voydan Pop Georgiev- Chernodrinski   \n18      0009801bd85e5806  The Mitsurugi point made no sense - why not ar...   \n19      0009eaea3325de8c  Don't mean to bother you \\n\\nI see that you're...   \n20      000b08c464718505  \"\\n\\n Regarding your recent edits \\n\\nOnce aga...   \n21      000bfd0867774845  \"\\nGood to know. About me, yeah, I'm studying ...   \n22      000c0dfd995809fa  \"\\n\\n Snowflakes are NOT always symmetrical! \\...   \n23      000c6a3f0cd3ba8e  \"\\n\\n The Signpost: 24 September 2012 \\n\\n Rea...   \n24      000cfee90f50d471  \"\\n\\nRe-considering 1st paragraph edit?\\nI don...   \n25      000eefc67a2c930f  Radial symmetry \\n\\nSeveral now extinct lineag...   \n26      000f35deef84dc4a  There's no need to apologize. A Wikipedia arti...   \n27      000ffab30195c5e1  Yes, because the mother of the child in the ca...   \n28      0010307a3a50a353  \"\\nOk. But it will take a bit of work but I ca...   \n29      0010833a96e1f886  \"== A barnstar for you! ==\\n\\n  The Real Life ...   \n...                  ...                                                ...   \n159541  ffa33d3122b599d6  Your absurd edits \\n\\nYour absurd edits on gre...   \n159542  ffa95244f261527f  maybe he's got better things to do than spend ...   \n159543  ffad104337fe9891  scrap that, it does meet criteria and its gone...   \n159544  ffaed63c487a2b42                                You could do worse.   \n159545  ffb268f37788a011  , 7 March 2011 (UTC)\\nAre you also User:Bmatts...   \n159546  ffb47123b2d82762  \"\\n\\nHey listen don't you ever!!!! Delete my e...   \n159547  ffb7b4c3d3ae5842                     Thank you very, very much.  ·✆   \n159548  ffb93b0a0a1e78f9                        Talkback: 15 September 2012   \n159549  ffb998f9749bd83e                         2005 (UTC)\\n 06:35, 31 Mar   \n159550  ffba5332d6b8fd14  i agree/ on another note lil wayne is a talent...   \n159551  ffbc2db4225258dd  While about half the references are from BYU-I...   \n159552  ffbcd64a71775e04  Prague Spring \\n\\nI think that Prague Spring d...   \n159553  ffbd331a3aa269b9  I see this as having been merged; undoing one ...   \n159554  ffbdbb0483ed0841  and i'm going to keep posting the stuff u dele...   \n159555  ffc2f409658571f1  \"\\n\\nHow come when you download that MP3 it's ...   \n159556  ffc671f2acdd80e1  I'll be on IRC, too, if you have a more specif...   \n159557  ffc7bbb177c3c966  It is my opinion that that happens to be off-t...   \n159558  ffca1e81aefc48ac  Please stop removing content from Wikipedia; i...   \n159559  ffca8d71d71a3fae  Image:Barack-obama-mother.jpg listed for delet...   \n159560  ffcdcb71854f6d8a  \"Editing of article without Consensus & Remova...   \n159561  ffd2e85b07b3c7e4  \"\\nNo he did not, read it again (I would have ...   \n159562  ffd72e9766c09c97  \"\\n Auto guides and the motoring press are not...   \n159563  ffe029a7c79dc7fe  \"\\nplease identify what part of BLP applies be...   \n159564  ffe897e7f7182c90  Catalan independentism is the social movement ...   \n159565  ffe8b9316245be30  The numbers in parentheses are the additional ...   \n159566  ffe987279560d7ff  \":::::And for the second time of asking, when ...   \n159567  ffea4adeee384e90  You should be ashamed of yourself \\n\\nThat is ...   \n159568  ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...   \n159569  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n159570  fff46fc426af1f9a  \"\\nAnd ... I really don't think you understand...   \n\n        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n0           0             0        0       0       0              0  \n1           0             0        0       0       0              0  \n2           0             0        0       0       0              0  \n3           0             0        0       0       0              0  \n4           0             0        0       0       0              0  \n5           0             0        0       0       0              0  \n6           1             1        1       0       1              0  \n7           0             0        0       0       0              0  \n8           0             0        0       0       0              0  \n9           0             0        0       0       0              0  \n10          0             0        0       0       0              0  \n11          0             0        0       0       0              0  \n12          1             0        0       0       0              0  \n13          0             0        0       0       0              0  \n14          0             0        0       0       0              0  \n15          0             0        0       0       0              0  \n16          1             0        0       0       0              0  \n17          0             0        0       0       0              0  \n18          0             0        0       0       0              0  \n19          0             0        0       0       0              0  \n20          0             0        0       0       0              0  \n21          0             0        0       0       0              0  \n22          0             0        0       0       0              0  \n23          0             0        0       0       0              0  \n24          0             0        0       0       0              0  \n25          0             0        0       0       0              0  \n26          0             0        0       0       0              0  \n27          0             0        0       0       0              0  \n28          0             0        0       0       0              0  \n29          0             0        0       0       0              0  \n...       ...           ...      ...     ...     ...            ...  \n159541      1             0        1       0       1              0  \n159542      0             0        0       0       0              0  \n159543      0             0        0       0       0              0  \n159544      0             0        0       0       0              0  \n159545      0             0        0       0       0              0  \n159546      1             0        0       0       1              0  \n159547      0             0        0       0       0              0  \n159548      0             0        0       0       0              0  \n159549      0             0        0       0       0              0  \n159550      0             0        0       0       0              0  \n159551      0             0        0       0       0              0  \n159552      0             0        0       0       0              0  \n159553      0             0        0       0       0              0  \n159554      1             0        1       0       1              0  \n159555      0             0        0       0       0              0  \n159556      0             0        0       0       0              0  \n159557      0             0        0       0       0              0  \n159558      0             0        0       0       0              0  \n159559      0             0        0       0       0              0  \n159560      0             0        0       0       0              0  \n159561      0             0        0       0       0              0  \n159562      0             0        0       0       0              0  \n159563      0             0        0       0       0              0  \n159564      0             0        0       0       0              0  \n159565      0             0        0       0       0              0  \n159566      0             0        0       0       0              0  \n159567      0             0        0       0       0              0  \n159568      0             0        0       0       0              0  \n159569      0             0        0       0       0              0  \n159570      0             0        0       0       0              0  \n\n[159571 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id               object\ncomment_text     object\ntoxic             int64\nsevere_toxic      int64\nobscene           int64\nthreat            int64\ninsult            int64\nidentity_hate     int64\ndtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_train.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_train = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.iloc[:limit_train, :2].values\n",
    "Y_train = df_train.iloc[:limit_train, 2:].values\n",
    "\n",
    "X_test = df_train.iloc[limit_train:, :2].values\n",
    "Y_test = df_train.iloc[limit_train:, 2:].values\n",
    "\n",
    "X_train = transform.split_comment(X_train)\n",
    "X_test = transform.split_comment(X_test)\n",
    "\n",
    "count, vocab = transform.mk_vocab(X_train + X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25385\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0000997932d777bf', array(['explanation', 'why', 'the', 'edits', 'made', 'under', 'my',\n       'username', 'hardcore', 'metallica', 'fan', 'were', 'reverted',\n       'they', 'weren', 't', 'vandalisms', 'just', 'closure', 'on',\n       'some', 'gas', 'after', 'i', 'voted', 'at', 'new', 'york', 'dolls',\n       'fac', 'and', 'please', 'don', 't', 'remove', 'the', 'template',\n       'from', 'the', 'talk', 'page', 'since', 'i', 'm', 'retired', 'now',\n       '89', '205', '38', '27'], dtype='<U11'))\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ident_train, X_train = transform.pass_data_to_word_idx(X_train, vocab, count)\n",
    "# ident_test, X_test = transform.pass_data_to_word_idx(X_test, vocab, count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400\n[array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 16,\n       34,  3, 35, 36,  3, 37, 38, 39, 24, 40, 41, 42, 43, 44, 45, 46])\n array([47, 48, 49, 50, 51, 52, 53, 24, 40, 54, 55, 56, 57, 37, 58, 59, 60,\n       61, 62, 63])\n array([64, 65, 24, 40, 66, 67, 68, 69, 70, 71, 72, 73, 18, 74, 51, 75, 76,\n       77, 78, 79, 80, 31, 81, 69, 82, 83,  4, 84, 85,  7, 37, 38, 49, 86,\n       69, 87, 88, 89,  3, 90, 91,  3, 92, 93])\n ...\n array([ 133,  104,  105, 7267,   85,  139,   74,   76,  108, 2848,  715,\n        133, 1130,   20,    7,   37,   38, 7749, 5812,  451,  214])\n array([ 8954,  2380,   128,    92,   176,   136,  6633,  2059, 15974,\n        3645])\n array([  31,   72, 1415,  384,   72,  185,  893,  133,  294, 1130,   20,\n          3,  279,   69,  134,    3,  132,  616,  275,   42,   74,   24,\n        383,   26,   72])]\n"
     ]
    }
   ],
   "source": [
    "max_len = transform.get_max_len_sent(X_train)#transform.get_max_len_sent(concatenate((X_train, X_test), axis=0))\n",
    "print(max_len)\n",
    "# print(concatenate((X_train, X_test), axis=0))\n",
    "print(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = transform.pad_sentences(X_train, max_len, vocab[transform.padding])\n",
    "# X_test = transform.pad_sentences(X_test, max_len, vocab[transform.padding])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.   2.   3. ...   0.   0.   0.]\n [ 47.  48.  49. ...   0.   0.   0.]\n [ 64.  65.  24. ...   0.   0.   0.]\n ...\n [173. 174.  69. ...   0.   0.   0.]\n [182. 101.   3. ...   0.   0.   0.]\n [218.  20.  51. ...   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159570, 1400)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0]\n(1000, 6)\n"
     ]
    }
   ],
   "source": [
    "# print(Y_test[0])\n",
    "# print(Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.conv_model import ConvModel\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from utils.cuda import use_cuda\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : loss = 0.086900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : loss = 0.055634\n"
     ]
    }
   ],
   "source": [
    "model = ConvModel(len(vocab), max_len)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "if use_cuda():\n",
    "    model.cuda()\n",
    "    loss_fn.cuda()\n",
    "\n",
    "opt = th.optim.Adagrad(model.parameters(), lr=5e-3)\n",
    "\n",
    "nb_epoch = 10\n",
    "\n",
    "batch_size = 32\n",
    "nb_batch = int(X_train.shape[0] / batch_size)\n",
    "\n",
    "for e in range(nb_epoch):\n",
    "    model.train()\n",
    "    \n",
    "    if use_cuda():\n",
    "        model.cuda()\n",
    "        \n",
    "    sum_loss = 0\n",
    "    nb_sent = 0\n",
    "    \n",
    "    for i in range(nb_batch):\n",
    "        i_min = batch_size * i\n",
    "        i_max = batch_size * (i + 1) if batch_size * (i + 1) < X_train.shape[0] else X_train.shape[0]\n",
    "                \n",
    "        x = th.Tensor(X_train[i_min:i_max]).long()\n",
    "        y = th.Tensor(Y_train[i_min:i_max])\n",
    "        if use_cuda():\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        sum_loss += loss.item()\n",
    "        nb_sent += 1\n",
    "    \n",
    "    sum_loss /= nb_sent\n",
    "    \n",
    "    print(\"Epoch %d : loss = %f\" % (e, sum_loss))\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    model.cpu()\n",
    "    \n",
    "    x_test = th.Tensor(X_test).long()\n",
    "    y_test = th.Tensor(Y_test)\n",
    "    res = th.zeros(y_test.size())\n",
    "    \n",
    "    batch_size_test = 32\n",
    "    nb_batch_test = int(x_test.size(0) / batch_size_test)\n",
    "    \n",
    "    for i in range(nb_batch_test):\n",
    "        i_min = batch_size_test * i\n",
    "        i_max = batch_size_test * (i + 1) if batch_size_test * (i + 1) < x_test.size(0) else x_test.size(0)\n",
    "        \n",
    "        x = x_test[i_min:i_max]\n",
    "        y = y_test[i_min:i_max]\n",
    "        \n",
    "        out_test = model(x)\n",
    "        # diff = th.abs(y - out_test)\n",
    "        res[i_min:i_max] = out_test\n",
    "    \n",
    "    diff = res.mean(dim=0)\n",
    "    differences = {\n",
    "        \"toxic\": diff[0].item(),\n",
    "        \"severe_toxic\": diff[1].item(),\n",
    "        \"obscene\": diff[2].item(),\n",
    "        \"threat\": diff[3].item(),\n",
    "        \"insult\": diff[4].item(),\n",
    "        \"identity_hate\": diff[5].item()}\n",
    "    y_test = y_test.detach().numpy()\n",
    "    res = res.detach().numpy()\n",
    "    roc_auc_scores = {\n",
    "        \"toxic\": roc_auc_score(y_test[:, 0], res[:, 0]),\n",
    "        \"severe_toxic\": roc_auc_score(y_test[:, 1], res[:, 1]),\n",
    "        \"obscene\": roc_auc_score(y_test[:, 2], res[:, 2]),\n",
    "        \"threat\": roc_auc_score(y_test[:, 3], res[:, 3]),\n",
    "        \"insult\": roc_auc_score(y_test[:, 4], res[:, 4]),\n",
    "        \"identity_hate\": roc_auc_score(y_test[:, 5], res[:, 5])\n",
    "    }\n",
    "    \n",
    "    print(roc_auc_scores)\n",
    "    print(sum([v for _, v in roc_auc_scores.items()]) / 6)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvModel(\n  (emb): Embedding(11154, 16)\n  (seq1): Sequential(\n    (0): Conv1d(16, 24, kernel_size=(5,), stride=(1,))\n    (1): ReLU()\n    (2): Conv1d(24, 32, kernel_size=(3,), stride=(1,))\n    (3): ReLU()\n    (4): MaxPool1d(kernel_size=1394, stride=1394, padding=0, dilation=1, ceil_mode=False)\n  )\n  (seq2): Sequential(\n    (0): Linear(in_features=32, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=6, bias=True)\n    (3): Sigmoid()\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "print(model)\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    "Save model, loss function, optimizer, vocabulary and word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = model.cpu()\n",
    "saved_loss = loss_fn.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open(\"saved/model.p\", \"wb\"))\n",
    "pickle.dump(loss_fn, open(\"saved/loss_fn.p\", \"wb\"))\n",
    "pickle.dump(opt, open(\"saved/opt.p\", \"wb\"))\n",
    "pickle.dump(vocab, open(\"saved/vocab.p\", \"wb\"))\n",
    "pickle.dump(count, open(\"saved/count.p\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
